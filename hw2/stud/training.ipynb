{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/andrea-gasparini/nlp-aspect-based-sentiment-analysis/blob/master/hw2/stud/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Aspect-Based Sentiment Analysis (ABSA)\n",
    "\n",
    "Second homework of the Natural Language Processing course 2021 @ Sapienza University of Rome.\n",
    "\n",
    "Prof. Roberto Navigli\n",
    "\n",
    "MSc in Computer Science\n",
    "\n",
    "**Author**: Andrea Gasparini - 1813486"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup the environment\n",
    "\n",
    "Note that this notebook can be directly executed in Colab by clicking the button above.\n",
    "\n",
    "Otherwise, it is supposed to be placed in the `hw2` directory before running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zJ0I8wtDoqfX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# if running on colab\n",
    "if 'google.colab' in sys.modules:\n",
    "\n",
    "\t# set up GitHub\n",
    "\tGITHUB_TOKEN = \"\"\n",
    "\tGITHUB_USER = \"andrea-gasparini\"\n",
    "\tassert GITHUB_TOKEN != \"\" and GITHUB_USER != \"\"\n",
    "\n",
    "\t# clone the repository from GitHub\n",
    "\t! git clone https://{GITHUB_USER}:{GITHUB_TOKEN}@github.com/andrea-gasparini/nlp-aspect-based-sentiment-analysis\n",
    "\t! mv nlp-aspect-based-sentiment-analysis/hw2/* .\n",
    "\t! mv nlp-aspect-based-sentiment-analysis/requirements.txt .\n",
    "\t! rm -rf nlp-aspect-based-sentiment-analysis\n",
    "\t! pip install -r requirements.txt --quiet --no-cache-dir\n",
    "\n",
    "\t# mount drive directories\n",
    "\tfrom google.colab import drive\n",
    "\tdrive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "\tROOT_DIR = '/content/drive/My Drive/data/nlp/hw2/'\n",
    "\tDATA_DIR = f'{ROOT_DIR}data/'\n",
    "\tMODELS_DIR = f'{ROOT_DIR}models/'\n",
    "\tEMBEDDINGS_DIR = f'{ROOT_DIR}embeddings/'\n",
    "\n",
    "else:\n",
    "\n",
    "\trelative_cwd_last_two = os.path.sep.join(os.getcwd().split(os.path.sep)[-2:])\n",
    "\tassert os.path.basename(relative_cwd_last_two) == \"hw2\",\\\n",
    "\t\tf\"This notebook is supposed to be runned only from \\\"hw2/\\\" or Google Colab, not from {relative_cwd_last_two}/\"\n",
    "\n",
    "\tROOT_DIR = '../'\n",
    "\tDATA_DIR = f'{ROOT_DIR}data/'\n",
    "\tMODELS_DIR = f'{ROOT_DIR}model/'\n",
    "\tEMBEDDINGS_DIR = f'{MODELS_DIR}embeddings/'\n",
    "\n",
    "assert os.path.isdir(ROOT_DIR), f\"{ROOT_DIR} is not a valid directory\"\n",
    "\n",
    "for dir in [DATA_DIR, MODELS_DIR, EMBEDDINGS_DIR]:\n",
    "\t! mkdir -p {dir.replace(' ', '\\ ')}\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Pre-trained static embeddings\n",
    "GLOVE_DIR = f\"{EMBEDDINGS_DIR}glove/\"\n",
    "GLOVE_FILENAME = \"glove.6B.300d.txt\"\n",
    "\n",
    "## Laptop datasets\n",
    "LAPTOP_TRAIN_JSON = os.path.join(DATA_DIR, \"laptops_train.json\")\n",
    "LAPTOP_DEV_JSON = os.path.join(DATA_DIR, \"laptops_dev.json\")\n",
    "assert os.path.isfile(LAPTOP_TRAIN_JSON), f\"{LAPTOP_TRAIN_JSON} does not contain a valid train dataset\"\n",
    "assert os.path.isfile(LAPTOP_DEV_JSON), f\"{LAPTOP_DEV_JSON} does not contain a valid development dataset\"\n",
    "\n",
    "## Restaurant datasets\n",
    "RESTAURANT_TRAIN_JSON = os.path.join(DATA_DIR, \"restaurants_train.json\")\n",
    "RESTAURANT_DEV_JSON = os.path.join(DATA_DIR, \"restaurants_dev.json\")\n",
    "assert os.path.isfile(RESTAURANT_TRAIN_JSON), f\"{RESTAURANT_TRAIN_JSON} does not contain a valid train dataset\"\n",
    "assert os.path.isfile(RESTAURANT_DEV_JSON), f\"{RESTAURANT_DEV_JSON} does not contain a valid development dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DyImy989ooPq",
    "outputId": "d11f87a5-f5ee-4da9-8e59-1d78b341c2b6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "import torchtext\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from evaluate import read_dataset\n",
    "\n",
    "from nltk import TreebankWordTokenizer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from stud.dataset import ABSADataModule\n",
    "from stud.pl_models import PlAspectClassifier\n",
    "from stud import utils, constants as const"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Reproducibility stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R6zuPRbfpQTG",
    "outputId": "9eb3e85b-e918-4437-c338-96ceca7d9e8d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "SEED = 42 # @param {type:\"integer\"}\n",
    "\n",
    "pl.seed_everything(SEED)\n",
    "torch.backends.cudnn.deterministic = True  # will use only deterministic algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Wandb logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mandreagasparini\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WANDB_PROJECT_AB = 'nlp_hw2-AB'\n",
    "WANDB_PROJECT_CD = 'nlp_hw2-CD'\n",
    "WANDB_KEY=\"\"\n",
    "wandb.login(key=WANDB_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/andrea/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Loading vectors from ../model/embeddings/glove/glove.6B.300d.txt.pt\n",
      "INFO: Loading vectors from ../model/embeddings/glove/glove.6B.300d.txt.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialised 7671 embeddings\n",
      "randomly initialised 851 embeddings\n"
     ]
    }
   ],
   "source": [
    "utils.nltk_downloads()\n",
    "\n",
    "data_module = ABSADataModule(read_dataset(LAPTOP_TRAIN_JSON) + read_dataset(RESTAURANT_TRAIN_JSON),\n",
    "                             read_dataset(LAPTOP_DEV_JSON) + read_dataset(RESTAURANT_DEV_JSON),\n",
    "                             tokenizer=TreebankWordTokenizer(),\n",
    "                             batch_size=8,\n",
    "                             num_workers=4,\n",
    "                             has_category=False)\n",
    "data_module.setup()\n",
    "\n",
    "if not os.path.isfile(f\"{GLOVE_DIR}{GLOVE_FILENAME}\"):\n",
    "    torchtext.vocab.GloVe(name=\"6B\", dim=300, cache=GLOVE_DIR)\n",
    "\n",
    "glove_embeddings = utils.load_pretrained_embeddings(GLOVE_FILENAME,\n",
    "                                                    GLOVE_DIR,\n",
    "                                                    data_module.vocabs[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MODE = \"ab\"\n",
    "BERT_PRETRAINED_PATH = (f\"{MODELS_DIR}bert-base-cased\"\n",
    "                        if os.path.isdir(f\"{MODELS_DIR}bert-base-cased\")\n",
    "                        else \"bert-base-cased\")\n",
    "\n",
    "label_key = utils.get_label_key(MODE)\n",
    "\n",
    "hparams = {\"vocab_size\": len(data_module.vocabs[\"text\"]),\n",
    "           \"hidden_dim\": 128,\n",
    "           \"embedding_dim\": glove_embeddings.shape[1],\n",
    "           \"pos_embedding\": False,\n",
    "           \"pos_embedding_dim\": 120,\n",
    "           \"pos_vocab_size\": len(data_module.vocabs[\"pos\"]),\n",
    "           \"bert_embedding\": True,\n",
    "           \"bert_finetuning\": False,\n",
    "           \"bert_layers_to_merge\": [-1, -2, -3, -4],\n",
    "           \"bert_layer_pooling_strategy\": \"mean\",\n",
    "           \"bert_wordpiece_pooling_strategy\": \"mean\",\n",
    "           \"bert_model_name_or_path\": BERT_PRETRAINED_PATH,\n",
    "           \"pack_lstm_input\": True,\n",
    "           \"label_vocab\": data_module.vocabs[label_key],\n",
    "           \"num_classes\": len(data_module.vocabs[label_key]),\n",
    "           \"bidirectional\": True,\n",
    "           \"num_layers\": 2,\n",
    "           \"dropout\": 0.5,\n",
    "           \"max_epochs\": 150,\n",
    "           \"attention\": False,\n",
    "           \"attention_heads\": 12,\n",
    "           \"attention_dropout\": 0.2,\n",
    "           \"attention_concat\": False,\n",
    "           \"attention_simple\": False,\n",
    "           \"mode\": MODE,\n",
    "           \"batch_size\": data_module.batch_size}\n",
    "\n",
    "if hparams[\"bidirectional\"]: MODEL_NAME = \"BiLSTM\"\n",
    "else: MODEL_NAME = \"LSTM\"\n",
    "\n",
    "MODEL_NAME += \" + GloVe\"\n",
    "if hparams[\"bert_embedding\"]:\n",
    "    MODEL_NAME += f\" + BERT_{hparams['bert_layer_pooling_strategy']}\"\n",
    "    if hparams[\"bert_finetuning\"]: MODEL_NAME += \" finetuned\"\n",
    "if hparams[\"pos_embedding\"]: MODEL_NAME += \" + POS\"\n",
    "if hparams[\"attention\"]:\n",
    "    MODEL_NAME += \" + attention\"\n",
    "    if not hparams[\"attention_simple\"]: MODEL_NAME += \" transformer\"\n",
    "    if hparams[\"attention_concat\"]: MODEL_NAME += \" + concat\"\n",
    "\n",
    "MODEL_NAME = f\"{MODE}_{MODEL_NAME}\"\n",
    "\n",
    "print(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "early_stopping = pl.callbacks.EarlyStopping(monitor='valid_aspect_polarity_classification_f1',\n",
    "                                            patience=10,\n",
    "                                            verbose=True,\n",
    "                                            mode='max')\n",
    "\n",
    "check_point_callback = pl.callbacks.ModelCheckpoint(monitor='valid_aspect_polarity_classification_f1',\n",
    "                                                    verbose=True,\n",
    "                                                    save_top_k=2,\n",
    "                                                    save_last=False,\n",
    "                                                    mode='max',\n",
    "                                                    dirpath=MODELS_DIR,\n",
    "                                                    filename=MODEL_NAME + '-{epoch}-{valid_loss:.4f}-{valid_aspect_identification_f1:.3f}-{valid_aspect_polarity_classification_f1:.3f}')\n",
    "\n",
    "wandb_logger = WandbLogger(offline=False, project=WANDB_PROJECT_AB, name=MODEL_NAME)\n",
    "\n",
    "trainer = pl.Trainer(gpus=1 if torch.cuda.is_available() else 0,\n",
    "                     logger=wandb_logger,\n",
    "                     val_check_interval=1.0,\n",
    "                     max_epochs=hparams[\"max_epochs\"],\n",
    "                     callbacks=[early_stopping, check_point_callback])\n",
    "\n",
    "model = PlAspectClassifier(hparams,\n",
    "                           embeddings=glove_embeddings,\n",
    "                           ignore_index=const.PAD_INDEX)\n",
    "trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/miniconda3/envs/nlp2021-hw2/lib/python3.7/site-packages/pytorch_lightning/utilities/parsing.py:262: UserWarning: Attribute 'label_vocab' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['label_vocab'])`.\n",
      "  f\"Attribute {k!r} is an instance of `nn.Module` and is already saved during checkpointing.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention': False,\n",
      " 'attention_concat': False,\n",
      " 'attention_dropout': 0.2,\n",
      " 'attention_heads': 12,\n",
      " 'attention_simple': True,\n",
      " 'augment_train': False,\n",
      " 'batch_size': 8,\n",
      " 'bert_embedding': True,\n",
      " 'bert_finetuning': False,\n",
      " 'bert_layer_pooling_strategy': 'second_to_last',\n",
      " 'bert_layers_to_merge': [-1, -2, -3, -4],\n",
      " 'bert_model_name_or_path': '../model/bert-base-cased',\n",
      " 'bert_wordpiece_pooling_strategy': 'mean',\n",
      " 'bidirectional': True,\n",
      " 'dropout': 0.5,\n",
      " 'embedding_dim': 300,\n",
      " 'hidden_dim': 128,\n",
      " 'label_vocab': Vocab(),\n",
      " 'max_epochs': 150,\n",
      " 'mode': 'ab',\n",
      " 'num_classes': 10,\n",
      " 'num_layers': 2,\n",
      " 'pack_lstm_input': True,\n",
      " 'polarity_vocab': None,\n",
      " 'pos_embedding': False,\n",
      " 'pos_embedding_dim': 120,\n",
      " 'pos_vocab_size': 45,\n",
      " 'vocab_size': 8522}\n",
      "MODEL: ASPECT SENTIMENT + ASPECT EXTRACTION\n",
      "\n",
      "Aspect Extraction Evaluation\n",
      "\tAspects\t TP: 906;\tFP: 230;\tFN: 176\n",
      "\t\tprecision: 79.75;\trecall: 83.73;\tf1: 81.70\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\n",
      "Aspect Sentiment Evaluation\n",
      "\n",
      "\tALL\t TP: 661;\tFP: 479;\tFN: 425\n",
      "\t\t(m avg): precision: 57.98;\trecall: 60.87;\tf1: 59.39 (micro)\n",
      "\t\t(M avg): precision: 46.44;\trecall: 48.97;\tf1: 47.41 (Macro)\n",
      "\n",
      "\tpositive: \tTP: 361;\tFP: 134;\tFN: 182;\tprecision: 72.93;\trecall: 66.48;\tf1: 69.56;\t495\n",
      "\tnegative: \tTP: 206;\tFP: 181;\tFN: 96;\tprecision: 53.23;\trecall: 68.21;\tf1: 59.80;\t387\n",
      "\tneutral: \tTP: 89;\tFP: 146;\tFN: 127;\tprecision: 37.87;\trecall: 41.20;\tf1: 39.47;\t235\n",
      "\tconflict: \tTP: 5;\tFP: 18;\tFN: 20;\tprecision: 21.74;\trecall: 20.00;\tf1: 20.83;\t23\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.freeze()\n",
    "\n",
    "predictions = list()\n",
    "for batch in data_module.val_dataloader():\n",
    "    predictions += model.predict(batch)\n",
    "\n",
    "import evaluate\n",
    "\n",
    "if model.hparams.mode == \"ab\":\n",
    "    print('MODEL: ASPECT SENTIMENT + ASPECT EXTRACTION\\n')\n",
    "    evaluate.evaluate_extraction(data_module.val_samples, predictions)\n",
    "    print(\"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\\n\")\n",
    "    evaluate.evaluate_sentiment(data_module.val_samples, predictions)\n",
    "    print('-------------------------------------------------------\\n')\n",
    "elif model.hparams.mode == \"cd\":\n",
    "    print('MODEL: CATEGORY SENTIMENT + CATEGORY EXTRACTION\\n')\n",
    "    evaluate.evaluate_sentiment(data_module.val_samples, predictions, 'Category Extraction')\n",
    "    print(\"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\\n\")\n",
    "    evaluate.evaluate_sentiment(data_module.val_samples, predictions, 'Category Sentiment')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "AaLPufHJb0I6"
   ],
   "name": "training.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "07f2475cefd6eff624302e41d0f13830e78395fa329e997d5f6e1b280e6600c8"
  },
  "kernelspec": {
   "display_name": "nlp2021-hw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
